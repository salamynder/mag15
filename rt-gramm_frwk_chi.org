#+INCLUDE: "title.org"
\pagenumbering{gobble}
#+INCLUDE: "erklaerung.org"
\tableofcontents
\pagenumbering{roman}
\listoffigures
\lstlistoflistings
# \listoftables
* Eingrenzung des Themas
\pagenumbering{arabic}
Diese Arbeit beschreibt und evaluiert eine mit »Grammatical Framework« (GF) implementierte sogenannte Ressourcen-Grammatik für das Chinesische. Eine Hinführung zum Thema GF sowie in die Nutzung der »Resource Grammar Library« (RGL) von GF wird den Evaluationsprozess nachverfolgbar machen. 
- \cite{thompson_type_1991}
** Wozu ist GF gut? Wo liegen die Vorteile?
*** Klärung des Begriffs »multilinguale Applikationsgrammatik«
GF ist eine Programmiersprache zum Programmieren *multilingualer Applikationsgrammatiken*. Der Aspekt der Multilingualität bedeutet für die Erstellung einer Applikationsgrammatik, dass immer zwischen sog. *konkreten Grammatiken* und *abstrakten Grammatiken* unterschieden wird. Konkrete Grammatiken werden für die Übersetzung in und aus bestimmten Sprachen erstellt; abstrakte Grammatiken dienen als Dreh- und Angelpunkt für konkreten Grammatiken, indem der Programmierer mit der abstrakten Grammatik ein semantisches Modell vorgibt, an das die konkreten Grammatiken gebunden sind. Dieses semantische Modell bezieht sich auf eine bestimmte Sprachdomäne und somit auch auf einen bestimmten Anwendungsfall, der sich software-technisch als Applikation, also als ein Programm bzw. eine Anwendung für einen bestimmten Zweck, manifestiert.

Zum Beispiel könnte unser Vorhaben eine Applikationsgrammatik für einen Tierpark sein, der an bestimmten Aussichtsplätzen und auf gewissen Endgeräten, wie etwa Touchscreens, eine Applikation bereitstellt, die Informationen zu den Tieren in verschiedenen Sprachen bietet. Im Kontext der Sprachdomäne /Tierpark/ wäre es dann klar, dass es sich bei einem Strauß nicht um einen Blumenstrauß, sondern vielmehr um einen Vogelstrauß handeln muss. Der Programmierer als Domänenexperte hat dann die Verantwortung, den Sachverhalt, dass es sich bei dem Strauß um ein Tier handeln soll, in entsprechender Weise in der Applikationsgrammatik zu hinterlegen.

*** Applikationsgrammatik mit Beispiel-Code, Hinweis auf RGL
\infoBox[Info]{Die Einzelheiten zu den verschieden Begrifflichkeiten der Grammatiken werden ausführlich im Tutoriums-Kapitel abgehandelt. In dieser Sektion geht es dagegen vornehmlich darum, einen knappen aber informativen Einblick in die Leistungsfähigkeit des GF-Systems, insbesondere auch der »Resource Grammar Library« (RGL) zu geben.}

Betrachten wir dazu den Satz »der Vogelstrauß frißt Beeren«, dann können wir diesen folgendermaßen in einer konkreten Grammatik für das Deutsche formalisieren:
#+name: zoo1
#+CAPTION[ZooGer]: Ausschnitt aus der konkreten Grammatik ZooGer.gf (./example-code/Zoo/)
#+INCLUDE: "example-code/Zoo/ZooGer.gf" :lines "5-11" src bash
In diesem Ausschnitt der konkreten Grammatik sind die sog. Linearisierungs-Regeln der konkreten Grammatik abgebildet. Allgemein haben sie die Form:
#+BEGIN_SRC bash
Name_der_Regel ( Argumente ) = Zeichenkette
                       [ LHS | RHS ]
#+END_SRC
Das heißt, auf der linken Seite vom Gleichheitszeichen, der *LHS* (engl. left hand side), wird eine Name für eine Regel vergeben, darauf folgen mögliche Argumente, und rechts vom Gleichheitszeichen, der *RHS* (engl. right hand side), werden Zeichenketten angegeben, die produziert werden, wenn die dazugehörige Regel angewandt wird. Zeichenketten sind die fundamentale Datenstruktur in GF und stehen immer zwischen nicht-typographischen Anführungszeichen, i. e. \inlst$""$.

Die Regeln sind unterteilt in konstante Regeln (\inlst$VStrauß, Fressen, Beere$) und solche Regeln, die diese Konstanten untereinander in Beziehung setzen (\inlst$Pred$). Setzen wir die Konstanten als Argumente für \inlst$Pred$ ein, erhalten wir den gesuchten Beispielsatz:
#+BEGIN_SRC bash
Pred VStrauß Fressen Beere = "der" ++ "Vogelstrauß" ++ "frißt" ++ "Beeren" ;
#+END_SRC
Nehmen wir nun eine zweite Grammatik hinzu, die eine Übersetzung unseres Satzes in Chinesische bereitstellt.
#+name: zoo2
#+CAPTION[ZooChi]: Ausschnitt aus der konkreten Grammatik ZooChi.gf (./example-code/Zoo/)
#+INCLUDE: "example-code/Zoo/ZooChi.gf" :lines "5-9" src bash
Und auch hier können wir durch Einsetzen der Konstanten in die Kombinationsregel \inlst$Pred$ erkennen, was linearisiert wird:
#+BEGIN_SRC bash
Pred VStrauß Fressen Beere = "鸵鸟" ++ "吃" ++ "浆果" ;
#+END_SRC
Man beachte nun, dass die LHS der \inlst$Pred$-Regel sowohl in der chinesischen als auch in der deutschen Grammatik exakt dieselbe ist, nämlich \inlst$Pred VStrauß Fressen Beere$. Diese LHS kann also als Schnittstelle für die Übersetzung zwischen verschiedenen Sprachen dienen: Wir wissen, wenn wir das semantische Modell \inlst$Pred VStrauß Fressen Beere$ in einer konkreten Grammatik implementiert haben, dass uns linearisierbare Zeichenketten für das Chinesische und für das Deutsche zur Verfügung steht.

Zudem ist ein Ausbau der übersetzbaren Sprachen leichthin möglich: Wenn wir wissen, dass das semantische Modell den Anforderungen unserer Anwendung (Applikation) entspricht (unser Tierpark also allein aus Straußen besteht, die nichts anderes tun, als Beeren zu fressen), dann können wir ohne Skrupel weitere Sprachen in Form von konkreten Grammatiken hinzufügen. Wir werden im Tutoriums-Kapitel genau beleuchten, wie das semantische Modell in abstrakten Grammatiken definiert und in konkreten Grammatiken implementiert wird (nur letzteres haben die Ausschnitte jetzt gezeigt). Aber schon hier sollte klar sein, wie das semantische Modell (abstrakte Grammatik) als stabiler Dreh- und Angelpunkt fungiert.
**** Einfache Flexibilität: Permutation, Reduplikation, Suppression 
Bietet uns das semantischen Modells nun Stabilität der ausdrückbaren Bedeutungen, so erlauben uns die konkreten Grammatiken in ihrem Zuschnitt auf eine bestimmte Zielsprache Flexibilität. Die bisherigen Beispiele waren nur in dem Sinne flexibel, als sie zeigten, dass Zeichenketten für eine bestimmte Sprache konfigurierbar sind: In der deutschen Grammatik nutzen wir \inlst$Fressen = "frißt"$; in der chinesischen \inlst$Fressen = "吃"$. Auch bot die identische Hauptsatzstruktur beider Sprachen keinerlei Anlass über eine Flexibilität nachzudenken, die über das bloße Vokabular hinausgehen: Beide Sprachen greifen auf die Satzgliedstellung Subjekt-Prädikat-Objekt (S-P-O) zurück. Ganz anders wäre es dagegen im Lateinischen, wo durchgängig die Stellung Subjekt-Objekt-Prädikat (S-O-P) herrscht. Eine einfache Grammatik für das Lateinische, die diesen Zusammenhang sicherstellt, zeigt folgende Auflistung:
#+CAPTION[ZooLat]: Ausschnitt aus der konkreten Grammatik ZooLat.gf (./example-code/Zoo/)
#+INCLUDE: "example-code/Zoo/ZooLat.gf" :lines "5-13" src bash
Die Möglichkeit Zeichenketten durch Kombinationsregeln unterschiedlich anzuordnen gehört zu den grundlegenden Freiheiten, die GF auf Zeichenketten-Ebene offeriert. Weitere Möglichkeiten, die allein mit Zeichenketten operieren sind:
#+ATTR_LATEX: :options [style=multiline,itemsep=4pt,parsep=0pt,leftmargin=3cm]
- Reduplikation :: \inlst$Pred vstr fres beer = vstr ++ fres ++ fres ++ beer => "鸵鸟 吃 吃 浆果"$, zu Deutsch: »Der Strauß isst ein paar Beeren.« (Reduplikation des Verbs dient im Chinesischen dazu die zeitliche Ausdehnung der Handlung abzuschwächen [Attenuation][fn::\cite[149]{xiao_aspect_2004}])
- Suppression :: \inlst$Pred vstr fres beer = vstr ++ beer$ (Also durch die Aufnahme eines Arguments durch \inlst$Pred$, aber Nicht-Anwendung in der Zeichenketten-Linearisierung. Anwendungsfall wäre z. B. das Entfallen des direkten Artikels bei Übersetzung vom Deutschen ins Russische.)
**** Jenseits von Zeichenketten und Modularisierung: weitere Flexibilität
Eine zusätzlich Ebene der Flexibilität wird durch Datensätze und Tabellen (/records/ respektive /tables/, siehe Tutoriums-Kapitel, [[rectables]]) geschaffen. Sie dienen dazu morphologische und syntaktische Informationen einer Sprache in Konstanten zu kodieren. Beispielsweise können wir alle grammatischen Kasus und das grammatische Geschlecht des deutschen Lexems »Beere« in folgender Datenstruktur hinterlegen:
#+BEGIN_SRC bash
{s = table
       ParamX.Number
       [table ResGer.Case ["Beere"; "Beere"; "Beere"; "Beere"];
        table ResGer.Case ["Beeren"; "Beeren"; "Beeren"; "Beeren"]];
 g = ResGer.Fem;
}
#+END_SRC
Und natürlich lässt sich auch mittels Hilfsfunktionen, die Erstellung dieser syntaktischen Datenstrukturen, in vielen Fällen auch über eine Regel, automatisieren. So geht in folgender Auflistung...
#+BEGIN_SRC bash
  reg1N : (x1 : Str) -> Gender -> N = \beere,g -> 
    case <beere,g> of {
      <_,  Fem> => 
        let beere : Str = case beere of {_ + "e" => beere + "n" ; _ => beere + "en"} ; 
            beeren = beere
        in mk6N beere beere beere beere beere beeren g ;
#+END_SRC

**** Beispiel zur Resource Grammar Library (RGL)
dfadf

GF: multilingual, Applikationsgrammatiken?
TODO: Besonderes Augenmerk soll dabei auch auf den Fakt gelegt werden, dass die verschiedenen konkreten Grammatiken völlig frei voneinander eine Sprache beschreiben können.

-> pivot lang -> indirekte Übersetzung (Interlingua)\cite{carstensen_computerlinguistik_2004}: REFTODO: Seite! Gibt eine Darstellung von Interlingua bei MÜ und verweist auch auf Maegaard für das Thema Sackgasse bei Interlingua MÜ:
- TODO:\cite{maegaard_mlim:_2001}:»Ideally, systems will employ statistical techniques to augment linguistic insights, allowing the system builder, a computational linguist, to specify the knowledge in the form most convenient to him or her, and have the system perform the tedious work of data collection, generalization, and rule creation. Such collaboration will capitalize on the (complementary) strengths of linguist and computer, and result in much more rapid construction of MT systems for new languages, with greater coverage and higher quality. Still, how exactly to achieve this optimal collaboration is far from clear. Chapter 6 discusses this tradeoff in more detail.« Fragen die GF beantworten kann? Natürlich werden Regeln nicht von GF erzeugt und statistische Methoden werden auch nicht standardmaäßig beim Parsen benutzt. Der Abschnitt lässt sich also so nicht benutzen, sondern vielmehr geht aus auf die Stärken bei GF was Arbeitsteilung/Modularisierung einzugehen. Robusteres Parsing ein zweites Thema zu den Stärken.
* Relevanz von GF
** für die Maschinelle Übersetzung (MÜ)
** für das Chinesische
* Tutorium
** Präliminarien und Überblick
Die nun verwendeten Beispiel-Grammatiken liegen (wie auch dieser Text) in einem öffentlich zugänglichen Archiv und stehen so zum eigenen Experimentieren bereit.[fn:: https://github.com/salamynder/mag15 . Siehe auch zur Installation von GF auf verschiedenen Systemen: http://www.grammaticalframework.org/download/index.html . GF_LIB_PATH unter Windows setzen: http://www.grammaticalframework.org/~inari/gf-windows.html . Die von mir verwendeten GF-Version ist TODO.] 

Der Ablauf der Tutoriums-Sektion gliedert sich in die aufeinander aufbauende Exposition folgender Punkte:
#+ATTR_LATEX: :options [itemsep=0pt,parsep=0pt]
- Module und Regeln für Grammatiken :: Grundlegende Terminologie in Modulen, die zusammen eine multilinguale Grammatik bilden (abstrakte und konkrete Grammatiken)
- Interaktion mit den Grammatiken :: Demonstration der Grammatik (Übersetzung etc.) in der GF-Shell
- Erläuterung des Datenmodell :: Mit Hilfe des Gelernten und Beobachteten das Datenmodell erläutern[fn:: Mit der Erläuterung des Datenmodell wird gleichzeitig eine praxisnahe Einführung in die Verwendung von Typen in GF sowie die curryfizierte Funktionsanwendung (curried function application) gegeben. Dies hat nicht nur den für uns hier praktischen Vorteil, dass Fehlermeldungen beim Kompilieren der Grammatiken eingeordnet werden können, sondern bietet auch generell eine Grundlage für das Verständnis /Funktionaler Programmierung/ (FP). REFTODO: Sektion über FP-Geschichte?!]

** Module und Regeln
Die folgenden 3 GF-Module (Zero.gf, ZeroGer.gf, ZeroChi.gf in Auflistung \ref{mj1}), deren Inhalt nur der Übersicht halber nebeneinander abgedruckt wird, ermöglichen eine Übersetzung der in ihnen beschriebenen semantischen Einheiten vom Chinesischen ins Deutsche und umgekehrt:

#+name: mj1
#+CAPTION[Hallo GF]: 3 Module einer multilingualen Grammatik (./example-code/Zero/)
#+BEGIN_SRC bash
--Zero.gf:                   --ZeroGer.gf:                   --ZeroChi.gf:
abstract Zero = {            concrete ZeroGer of Zero = {    concrete ZeroChi of Zero = { 
  cat                          lincat                          lincat                      
    S;   -- sentence             S, NP, VP, V2 = Str;             S, NP, VP, V2 = Str;
    NP;  -- noun phrase
    VP;  -- verb phrase
    V2;  -- two place verb
  fun                          lin                             lin
    Pred : NP -> VP -> S;        Pred np vp = np ++ vp;           Pred np vp = np ++ vp;
    Compl : V2 -> NP -> VP;      Compl v2 np = v2 ++ np;          Compl v2 np = v2 ++ np;
    John, Mary : NP;             John = "Johann";                 John = "约翰";
    Love : V2;                   Mary = "Marie";                  Mary = "玛丽";
                                 Love = "liebt";                  Love = "爱";
}                            }                               }
#+END_SRC

Jedes der Module besteht zunächst aus einer Kopfzeile (dem Header, siehe Zeile 2 von \ref{mj1}), der angibt, ob es sich um eine abstrakte oder konkrete Grammatik handelt, gefolgt vom Namen des Moduls. Der Modulname entspricht dem Namen der Datei, in der sich das Modul befindet, ohne Dateiendung. So befindet sich etwa das abstrakte Modul ~Zero~ in der Datei ~Zero.gf~. Einzeilige Kommentare können mit ~--~, mehrzeilige mit ~{-~ und ~-}~ in den Quellcode geschrieben werden.[fn:: Die Zero-Grammatik ist eine Abwandlung der Beispiele in \cite{ranta_gf-lrec-2010.pdf_2010} sowie \cite{_grammatical_2014}.]

Nach dem Modul-Header wird der Modul-Hauptteil (der Body) mit »= {« eingeleitet und mit »}« abgeschlossen, wobei wir es innerhalb des Hauptteils mit verschiedenen Urteilen oder Regeln (judgements or rules) zu tun haben.[fn:: \cite[45]{ranta_grammatical_2011}] Abstrakte Module, wie ~Zero~, benutzen hauptsächlich zwei Arten von Regeln, die mit folgenden Schlüsselwörtern eingeleitet werden:
#+ATTR_LATEX: :options [itemsep=0pt,parsep=0pt]
- $\ast$ ~cat~ :: *Kategorien-Deklarationen*, die die Kategorien (Typen von Bäumen) beschreiben
- $\ast$ ~fun~ :: *Funktions-Typen-Deklarationen*, die die verschiedenen Kategorien miteinander in Beziehung setzen

Konkrete Module, wie ~ZeroGer~ und ~ZeroChi~, haben im Header nach dem Schlüsselwort ~conrete~ ebenso ihren Namen, müssen aber auch noch anführen, auf welche abstrakte Grammatik sie sich beziehen. In unserem Fall zeigt ~of Zero~ an, dass sie sich auf das 
Modul in der Datei Zero.gf beziehen. Daraufhin beginnt auch hier der Modul-Body, in dem wir auch wieder zwei Arten von Regeln vorfinden:
#+ATTR_LATEX: :options [itemsep=0pt,parsep=0pt]
- $\ast$ ~lincat~ :: *Linearisierungs-Typ-Definitionen*, die den Typ der Ausgabe-Objekte des \inlst$linearize$-Kommandos definieren
- $\ast$ ~lin~ :: *Linearisierungs-Regeln* für Kategorien

Man beachte die Korrespondenz zwischen den ~fun~-Regeln im abstrakten Modul und den verschiedenen ~lin~-Regeln der konkreten Grammatiken: Zum Beispiel wird mit dem Ausdruck ~Compl~ in der abstrakten Grammatik eine Funktion mit dazugehöriger Typen-Deklaration hinter dem Doppelpunkt eingeführt. Die Typen-Deklaration beschreibt, welchen Typ die Objekte haben dürfen, die die Funktion ~Compl~ als Argumente nimmt.
#+BEGIN_SRC bash
abstract Zero = {             concrete Zero___ of Zero = {
[...]
    Compl : V2 -> NP -> VP;      Compl v2 np = v2 ++ np; -- ident. Definition in
[...]                                                    -- ZeroChi und ZeroGer
}                             }
#+END_SRC
Im Beispiel von ~Compl~ etwa bedeutet die Typen-Deklaration \inlst$V2 -> NP -> VP;$, dass zwei Argumente, zunächst vom Typ V2 und dann vom Typ NP, benötigt werden, um ein Objekt vom Typ VP zu erzeugen. Wir sagen, die Argument-Typen (/argument type/) der Funktion ~Compl~ sind V2 und NP und der Wert-Typ (/value type/) derselben ist VP. Der Sachverhalt, wieviele Argumente eine Funktion nimmt, wird mathematisch auch mit den Begriff der Stelligkeit bzw. Arität beschrieben; ~Compl~ hat daher eine Arität von zwei.

Welche konkreten Objekte dann mit dieser abstrakten Typen-Deklaration verbunden werden, ist ersichtlich aus den Linearisierungs-Regeln der konkreten Grammatiken. Die beiden konkreten Grammatiken ZeroChi und ZeroGer haben in unserem Beispiel dieselbe Definition von ~Compl~, nämlich \inlst$Compl v2 np = v2 ++ np;$. Die Komponenten dieser Definition sind:
#+ATTR_LATEX: :options [style=multiline,itemsep=4pt,parsep=0pt,leftmargin=3cm]
- Compl :: der Name der Funktion für die Funktions-Definition, der mit dem Namen der Funktions-Typ-Deklaration übereinstimmen muss
- v2 /und/ np :: die Argumente der Funktions-Definition, die prinzipiell beliebig gewählt werden dürfen, die aber hier namentlich darauf hinweisen, dass das Objekt, das sie referenzieren, vom Typ V2 bzw. NP ist
- = :: das Gleichheitszeichen leitet den Funktionskörper (/funtion body/) ein
- v2 ++ np :: hier werden die beiden Argumente der ~Compl~-Funktion mittels ~++~-Operator verkettet (String-Konketenation)
- ; :: wie jede Regel bzw. Urteil wird der Funktionskörper mit einem Simikolon beendet

** Interaktion in der GF-Shell
Die GF-Shell bietet über verschiedene Kommandos einen interaktiven Zugang zu den Grammatiken.[fn:: Für eine ausführlichere Einführung in die Arbeit mit der Shell, siehe http://www.grammaticalframework.org/doc/tutorial/gf-tutorial.html (etwas in die Jahre gekommen, aber die grundlegenden Ausführungen zur Shell und viele weitere Dinge sind noch aktuell) sowie \cite[31]{ranta_grammatical_2011}.] Um die Grammatiken der letzten Sektion in der Shell zu testen, müssen wir zunächst die Grammatik-Module laden, was am einfachsten geschieht, wenn wir zunächst in das Verzeichnis navigieren, wo sich dieselben befinden. Im jeweiligen Kommandozeilen-Interpreter[fn:: Unter Windows sollten standardmäßig zwei Shells zur Verfügung stehen: cmd.exe und Powershell; (Mac) OS X: iTerm] des Betriebssystems sollte nach der Installation von GF ein ~gf~-Programm verfügbar sein, sodass wir die GF-Shell damit starten können. Beim Aufruf kann man entweder gleich die betreffenden Grammatik-Modul-Dateien angeben (\inlst$>gf ZeroChi.gf ZeroGer.gf$), sodass man mit den geladenen Grammatiken arbeiten kann:

#+BEGIN_LATEX
\vspace{5mm}
\lstset{basicstyle=\small\ttfamily\setstretch{0.7},caption=Direkter Aufruf von Modulen durch das gf-Programm,numbers=none,language=bash}
\begin{lstlisting}
~/d/n/e/Zero $ gf ZeroChi.gf ZeroGer.gf

         *  *  *
      *           *
    *               *
   *
   *
   *        * * * * * *
   *        *         *
    *       * * * *  *
      *     *      *
         *  *  *

This is GF version 3.7-darcs. 
No detailed version info available
Built on linux/x86_64 with ghc-7.10, flags: interrupt server
License: see help -license.   

linking ... OK

Languages: ZeroChi ZeroGer
Zero> 
\end{lstlisting}
%\vspace{5mm}
#+END_LATEX

Oder man startet die GF-Shell einfach mit ~gf~ (also ohne jedwede Datei-Argumente) und importiert dann mittels des ~import~-Kommandos von der GF-Shell aus die Modul-Dateien:

#+BEGIN_LATEX
\vspace{5mm}
\lstset{basicstyle=\small\ttfamily\setstretch{0.7},caption=,numbers=none,language=bash}
\begin{lstlisting}
> import ZeroChi.gf ZeroGer.gf
linking ... OK

Languages: ZeroChi ZeroGer
6 msec
Zero> 
\end{lstlisting}
%\vspace{5mm}
#+END_LATEX

In jedem Fall sollte es dann möglich sein, folgende Kommando-Kombination auszuführen:

#+name: pl1
#+CAPTION[Übersetzungskommandos in der GF-Shell]: GF-Shell: Standard-Übersetzungs-Kommandoabfolge: ~parse~, »|« (Pipe), ~linearize~   
#+BEGIN_SRC bash
Languages: ZeroChi ZeroGer
Zero> parse -lang=ZeroChi "约翰 爱 玛丽" | linearize -lang=ZeroGer
Johann liebt Marie
#+END_SRC

In Zeile 1 von Auflistung \ref{pl1} sehen wir die geladenen konkreten Grammatiken, die wir für eine Übersetzung heranziehen können hinter dem Label »Languages«. Sie bilden also den Geltungsbereich (engl. /scope/) für die Arbeit in der Shell. Zeile 2 beginnt mit dem sogenannten Prompt, der sich aus dem Namen der geladenen abstrakten Grammatik (sofern geladen) sowie einer nach rechts ausgerichteten Spitzklammer zusammensetzt. Nach dem Prompt können wir unsere Eingaben tätigen. In der angesprochenen Auflistung ist die Eingabe eine Kombination von Kommandos, die eine Chinesisch-Deutsch-Übersetzung bewerkstelligt.

Im einzelnen werden dafür zwei Kommandos, ~parse~ und ~linearize~, und ein Operator benötigt, der die Ausgabe des ersten Kommandos an das zweite weiterleitet. Der genaue Ablauf sieht folgendermaßen aus:

#+ATTR_LATEX: :options [itemsep=0pt,parsep=0pt]
1. Eine chinesische Zeichenkette oder auch String (\inlst$"约翰 爱 玛丽"$) wird mittels ~parse -lang=ZeroChi~ eingelesen und verarbeitet.[fn:: Man beachte, dass ein String, der eingelesen werden soll, in Anführungszeichen eingeschlossen sein muss. Obligatorisch ist außerdem, die einzelen Wörter (Tokens) im String durch ein Leerzeichen zu trennen. Um dies /hervorzuheben/ wird in den Auflistungen das Leerzeichen in Strings als ␣ (U+2423, Open Box) umgesetzt. (GF ist in erster Linie kein Werkzeug zur Tokenisierung chinesischer Sätze. Siehe auch: TODO: cf. Chinesisch-Tokenisierung-Problem.)]
2. Das Ergebnis der Verarbeitung wird durch den sog. Pipe-Operator, ~|~, weitergeleitet \ldots{}
3. \ldots{} an ~linearize~, das eine deutsche Übersetzung mittels ~-lang=ZeroGer~ in Zeile 3 generiert.

# \infoBox[Hilfe?]{Zu allen Kommandos ist eine Hilfe per \verb~help~ abrufbar. So liefert \inlst$help parse$ beispielsweise eine Übersicht über das \verb~parse~-Kommando.}

Aus der Beobachtung dieses Ablaufs ergeben sich mindestens zwei Fragen:
#+ATTR_LATEX: :options [itemsep=0pt,parsep=0pt]
1. Was genau wird von dem Pipe-Operator weitergeleitet?
2. Wie genau steht dieser Ablauf im Verhältnis zu den von uns oben angeführten drei Grammatiken? (Auflistung \ref{mj1})

Frage 1 können wir oberflächlich in der Shell beantworten, indem wir den Pipe-Operator und ~linearize~ weglassen. 
#+name: mj-hello-ast
#+CAPTION[AST: Klammernotation]: AST in Klammernotation
#+BEGIN_SRC bash
Zero> parse -lang=ZeroChi "约翰 爱 玛丽"
Pred John (Compl Love Mary)
#+END_SRC

Um diese Ausgabe zu interpretieren und auch die zweite Frage zu beantworten, müssen wir uns eingehend mit Bäumen als Datenstrukturen auseinandersetzen.
** Datenmodellierung und Prüfung
*** Abstrakter Syntax Baum/Tree (AST)
Was ~parse~ in Auflistung \ref{mj-hello-ast} zurück liefert, sind die semantischen Einheiten unseres geparsten Satzes als sog. /Abstrakter Syntax Baum/ (Abstract Syntax Tree, kurz AST) in /Klammernotation/. Diese Notation lässt nicht intuitiv vermuten, dass es sich bei \inlst$Pred John (Compl Love Mary)$ um eine Art Baum handelt. (Obwohl die Klammern um ~Compl Love Mary~ wie in einer mathematischen Gleichung einen Hinweis darauf geben, dass etwas, nämlich ein geklammerter Ausdruck, zuerst berechnet werden muss.) Um uns nun diesen Vergleich mit einer Baum-Struktur zu verdeutlichen, können wir das GF Kommando ~visualize_tree~ in Verbindung mit dem Visualisierungs-Werkzeug »Graphviz«[fn:: Siehe http://www.graphviz.com] einsetzen:
#+BEGIN_SRC bash
Zero> parse -lang=ZeroChi "约翰 爱 玛丽" | visualize_tree -view="firefox"
#+END_SRC
Damit sollte sich ein Programm unserer Wahl (hier der Firefox-Browser) mit der PNG-Bilddatei öffnen, das uns einen auf den Kopf gestellten Baum zeigt:

# :float t -> center image!
#+CAPTION[vt-1]: ~visualize\_tree~ produziert Graphen-Darstellung eines AST (»Abstact Syntax Tree«)
#+NAME: jlm-abs-graph
#+ATTR_LATEX: :width 0.35\textwidth :float t
[[./example-code/Zero/1-JohannesLiebtMarie.png]]
Nun sollte ersichtlich sein, was gemeint ist, wenn wir dem Ausdruck \inlst$Pred John (Compl Love Mary)$ eine Baumstruktur zusprechen: Die Wurzel eines Baumes ist Ausgangspunkt für verschiedene Äste, die zu unterschiedlichen Blättern führen. Im obigen Fall ist die Wurzel nun ~Pred~ von der ausgehend Äste zum Subjekt, ~John~, und zum Prädikat (~Compl~ \ldots{}) wachsen, wobei sich ~Compl~ wiederum verzweigt in ~Love~ und ~Mary~.[fn:: (~Pred~: die Prädikation, TODO: Hadumot Bußmann Quelle; Ziel: P. müsste meinen, einem Gegenstand Qualitäten zu oder absprechen! TODO: Fußnote oder Infobox zu semantischen/syntaktischen Bezeichnern!)][fn:: Abkürzung für Prädikation (engl. /predication/), cf. REFTODO-Seite noch nicht richtig: \cite{busmann_lexikon_2008}.]

Vergleichen wir nun diesen Graphen mit unserem abstrakten Syntaxmodul (Zero.gf), so zeigt sich eine Übereinstimmung zwischen den geparsten semantischen Einheiten des AST und den Namen der Funktions-Deklarationen im ~fun~-Block:
#+name: mjAbs
# +CAPTION[Hello-Abs]
#+begin_src bash
abstract Zero = {
  cat                       -- Kategorien
    S; NP; VP; V2;
  fun                       -- Beginn des fun-Blocks
    Pred : NP -> VP -> S;
    Compl : V2 -> NP -> VP;
    John, Mary : NP;
    Love : V2;
}
#+end_src
Die jeweils durch ein Semikolon getrennten Funktionen im ~fun~-Blocks geben an, wie die verschiedenen Kategorien des ~cat~-Blocks produziert werden. Dies geschieht über sog. Typen-Deklarationen hinter dem Doppelpunkt. \inlst$Pred : NP -> VP -> S;$ bedeutet etwa, dass eine Funktion namens ~Pred~ zwei Argumente nimmt, zunächst eines vom Typ ~NP~ (Nominalphrase) und dann eines vom Typ ~VP~ (Verbphrase), um schließlich ein Objekt vom Typ ~S~ (Sentence) zu produzieren. Zur Erinnerung ist diese erste Funktion oder Regel in Abbildung \ref{jlm-eval-graph} /rechteckig/ umrandet.[fn:: TODO: Funktion oder Regel: logisch/semantisch?!]

#+CAPTION[eval-1]: Evaluations-Reihenfolge
#+NAME: jlm-eval-graph
#+ATTR_LATEX: :width 0.35\textwidth :float t
[[./example-code/Zero/1-JohannesLiebtMarie-Eval-Order.png]]

Abbildung \ref{jlm-eval-graph} macht aber auch klar, dass das zweite Argument von ~Pred~ (vom Typ ~VP~) sich nun wiederum aus zwei Komponenten zusammensetzt, was im Bild trapezförmig markiert ist und durch die Regel \inlst$Compl : V2 -> NP -> VP;$ in der abstrakten Grammatik beschrieben wird. Damit können wir jetzt auch die Parallele zur Klammernotation ziehen und sehen, dass mit ihr wirklich sehr kompakt der gesamte Baum beschrieben wird. So besagt \inlst$Pred John (Compl Love Mary)$, dass zunächst die Funktion ~Pred~ ihre erstes Argument ~John~ (vom Typ ~NP~) zugespielt bekommt und dass dann aber -- um das zweite Argument für ~Pred~ zu erhalten -- vorrangig die Funktion ~Compl~ mit ihren eigenen Argumenten, ~Love~ und ~Mary~, abgearbeitet oder evaluiert werden muss. Und gerade diese Vorrangigkeit oder /Präzedenz/ der Evaluation wird mit den runden Klammern um ~Compl Love Mary~ beschrieben.
*** Prüfung des Datenmodells: Type Checking
Die Relation zwischen Funktions-Anwendung (engl. /function application/, das Befüllen oder Sättigen einer Funktion mit ihren Argumenten) im AST und den Kategorien/Typen können wir auch sehr gut in der Shell illustrieren: Wir füttern dafür das Kommando ~linearize~ (das ja einen AST nimmt, um einen String zu produzieren) mit unvollständigen Bäumen und beobachten was passiert.

#+CAPTION: Typ-Fehler: keine Argumente
#+NAME: compl-no-args
#+BEGIN_SRC bash
Zero> linearize Pred John Compl
Couldn't match expected type VP
       against inferred type V2 -> NP -> VP
In the expression: Compl
#+END_SRC
Hier sehen wir nach dem Aufruf von ~linearize~ mit einem unvollständigen AST, wie die Linearisierung fehlschlägt und demzufolge kein String ausgegeben wird. Stattdessen teilt uns der Typ-Checker (engl. /type checker/) mit, dass der von uns bereitgestellte AST nicht den von uns in der Grammatik formulierten Erwartungen entspricht. Insbesondere bereitet der Ausdruck ~Compl~ Probleme, dessen Typ nicht mit jenem übereinstimmt, der als zweites Argument von ~Pred~ erwartet wird. Zur Erinnerung:
#+ATTR_LATEX: :options [itemsep=0pt,parsep=0pt]
- \inlst$Pred : NP -> VP -> S;$
- \inlst$Compl : V2 -> NP -> VP;$
~Pred~ erwartet ein Objekt vom Typ ~VP~ als zweites Argument; der Ausdruck ~Compl~ ist aber als Funktion noch vollkommen ungesättigt -- ihm wurden also noch keine Argumente übergeben --, weswegen der Compiler den Typ vollkommen korrekt als \inlst$V2 -> NP -> VP$ ableitet oder inferiert (Zeile 3), was aber eben laut Typen-Definition nicht das zweite Argument von ~Pred~ sein kann. Daher der ausgegebene Typ-Fehler (type error) und der Abbruch des Kommandos. Beachten wir hingegen die Präzedenz-Klammerung (die runden Klammern sind also zwingend notwendig) und sättigen ~Compl~ mit allen notwendigen Ausdrücken (Funktionsargumenten), bekommen wir natürlich die Linearisierung unseres AST als Strings:

#+CAPTION[AST: Komplette Funktionsanwendung]: Kein Typ-Fehler: Funktion ~Compl~ vollständig mit Argumenten gesättigt
#+NAME: compl-all-args
#+BEGIN_SRC bash
Zero> linearize Pred John (Compl Love Mary)
约翰 爱 玛丽
Johann liebt Marie
#+END_SRC

Damit hätten wir die Fälle gezeigt, in denen die ~Compl~-Funktion, entweder keine Argumente erhält (Abb. \ref{compl-no-args}) oder alle (Abb. \ref{compl-all-args}). Der Vollständigkeit halber sei auch noch gezeigt, dass eine partielle Sättigung der Funktion (im Fall von ~Compl~ also mit nur einem Argument) möglich ist und wie dieser Fall vom Compiler interpretiert wird:
#+caption[AST: Partielle Funktionsanwendung]: Typ-Fehler: Funktion ~Compl~ partiell gesättigt
#+NAME: compl-part-args
#+BEGIN_SRC bash
Zero> linearize Pred John (Compl Love)
Couldn't match expected type VP
       against inferred type NP -> VP
In the expression: Compl Love
#+END_SRC
In Auflistung \ref{compl-part-args} wird die Funktion ~Compl~ auf ein Argument vom Typ ~V2~ (Verb mit Platz für zwei Objekte: Subjekt und Objekt, ~Love~) angewandt, was für den Typ-Inferenz-Mechanismus des Compilers laut Zeile 3 bedeutet, dass der Ausdruck ~Compl Love~ den Typ ~NP -> VP~ besitzt (\inlst$Compl Love : NP -> VP$). Auf diese spezielle Art der Funktionsanwendung, die partielle Applikation, sei an dieser Stelle schon hingewiesen, weil hiermit ein Merkmal funktionaler Programmiersprachen expliziert wird, das uns auch in GF als FP immer wieder begegnen wird: Jeder Ausdruck, den wir benutzen, um etwas zu berechnen oder um Daten zu modellieren, ist eine Funktion. Vielfach nehmen Funktionen zwar Argumente entgegen und verarbeiten diese, wie etwa ~Compl~ in der Zero-Grammatik:

#+BEGIN_SRC bash
    Compl : V2 -> NP -> VP;      Compl v2 np = v2 ++ np;
#+END_SRC

Andere Ausdrücke, die keine Argumente nehmen, sind aber ebenso Funktionen, wie: 

#+BEGIN_SRC bash
    Love : V2;                   Love = "爱";
    John : NP;                   John = "约翰";
    Mary : NP;                   Mary = "玛丽";
#+END_SRC





Außerdem zeigt uns der Graph die Kategorien (~cat~)
#+BEGIN_SRC bash 
Zero> linearize Pred John Compl
Couldn't match expected type VP
       against inferred type V2 -> NP -> VP
#+END_SRC

#+begin_src bash
abstract Zero = {           concrete ZeroChi of Zero = {
  cat                         lincat
    S; NP; VP; V2;               S, NP, VP, V2 = Str;
  fun                         lin
    Pred : NP -> VP -> S;        Pred np vp = np ++ vp;
    Compl : V2 -> NP -> VP;      Compl v2 np = v2 ++ np;
    John, Mary : NP;             John = "约翰";
    Love : V2;                   Mary = "玛丽";
                                 Love = "爱";
}                           }
#+end_src

#+BEGIN_SRC bash
Zero> parse -lang=ZeroChi "约翰 爱 玛丽" | visualize_tree -view="firefox"
Pred John (Compl Love Mary)
#+END_SRC

#+CAPTION[tabpress]: Inkrementelles Parsing und Vorschläge für das 
#+NAME: jlm-tab
#+BEGIN_SRC bash 
Zero> linearize Pred 
Compl  John   Love   Mary   Pred -- Warum wird Compl vorgeschlagen?! -- das ist kein richtiges Inkrementelles Parsing; klar ist ja auch linearize... :P
#+END_SRC

** Notizen über das verwendete Vokabular
- Frege, Curry, Schönfinkel

- Angelov, 5: cat sind abstrakte syntaktische Kategorien (syntaktische Aspekt des Frameworks); sind gleichzeitig Martin Löfs basale Typen
- fun This,That,These,Those : Kind → Item; (grammatically this and that are determiners; *logically* they are functions)


** Records und Tables
<<rectables>>
Eyes:
- Agreement is indeed assumed to be one of the strengths of GF, so it is important to understand how it works! And not difficult, if you start with simple examples. Yours is simple enough, so let's look at it.

I have put a minimal grammar in

  http://cloud.grammaticalframework.org/gfse/

entitled "Eyes", and you can play with it and extend it as you want. The main idea is that

- NP has Number as inherent feature (field in a record)
- N has Number as variable feature (argument in a table)

Determiners set the Number of an NP, and select the number of N. Thus »this« sets an NP to be Sg, and selects the Sg form of the N.

With "your", you must think in a bit tricky way. There are, so to say, two variants of it: YourSg and YourPl. Many languages actually differentiate them (e.g. French and German) but in English they are the same string. But otherwise they work like This and These.

You should read the GF book chapter 3 for more details, and then 4 and 9 for even more details. If you don't have the book, the book slides may give enough information.

** opers
- lambda function abstraction: \cite[1]{nederpelt_type_2014} : dummy variable
** für Erklärung von Begriffen evtl. relevant
- synkategorematisch: GF: 100 (This), Granström: 7

* Evaluation der chinesischen Ressourcen Grammatik
** eng_chi2.txt:
*** 把 nur auf Dinge beziehbar?
- mkUtt (mkVP answer_V2S he_NP (mkS (mkCl she_NP sleep_V))) 
to answer to him that she sleeps
把他回答说她睡 BAD 回答他说她睡了
- mkUtt (mkVP (mkVPSlash paint_V2A (mkAP black_A)))
to paint itself black
画自己黑 BAD 把它自己画黑


** Komplement des Resultats (结果补语) -- shi-de -- »Buch ist ausverkauft«?
- es scheint noch nichts dafür definiert zu sein
- versuche Satz zu bilden: "Dieses Buch ist ausverkauft"
- ~/d/n/G/l/s/chinese git:master ❯❯❯
- gf AllChi.gfo
- AllChiAbs> p "这 本 书 卖 光 " => The sentence is not complete
- tab comletion after guang -> guang hua 光滑:
LexiconChi.gf
182:smooth_A = mkA "光滑" ;

sysu/Assign_4.gf
425:glaze_V = mkV "变得光滑" ; -- 1

sysu/Assign_6.gf
27:glossy_A = mkA "光滑" ; -- 7

- Satz müsste eher mit 售完 gebildet werden! (noch nicht in RGL-Chi)
- und dann ist auch die Frage, ob shi...de dafür benutzt wird, wahrscheinlich schon: 这本书是售完的. (Beschreibung Motsch, S. 127: "Betonung der Eigenschaft des Beschriebenen"), es geht aber auch: »这本书已售完« (Shanghai Dt-Chin., 134)

** 
* End
\printbibliography
* zotero							   :noexport:
# Local Variables:
# zotero-collection: #("4" 0 1 (name "ChinGrammar"))
# End:
# zotero-collection: #("4" 0 1 (name "ChinGrammar"))
# Ende:
* Header							    :ARCHIVE: :noexport:
#+TODO: TODO | WAITING DONE
#+LATEX_CLASS: cn-article
#+TITLE:
# leaving +TITLE blank -> no \maketitle generated by org
# Grundlagen maschineller multilingualer Übersetzung anhand des »Grammatical Framework« (GF) mit besonderer Berücksichtigung des Hoch-Chinesischen
# +AUTHOR: René Tobner
#+LANGUAGE: de-de
#+OPTIONS: H:4 skip:nil ^:nil timestamp:nil toc:nil

#+LATEX_HEADER: \usepackage[ngerman]{babel}
#+LATEX_HEADER: \addbibresource{mag.bib}

#+LATEX_HEADER: % Make commands for the quotes
#+LATEX_HEADER: \newcommand{\mq}[1]{\enquote{#1}}
#+LATEX_HEADER: \newcommand*{\openquote}{\tikz[remember picture,overlay,xshift=-15pt,yshift=-10pt]
#+LATEX_HEADER:      \node (OQ) {\quotefont\fontsize{60}{60}\selectfont``};\kern0pt}
#+LATEX_HEADER: \newcommand*{\closequote}{\tikz[remember picture,overlay,xshift=15pt,yshift=10pt]
#+LATEX_HEADER:      \node (CQ) {\quotefont\fontsize{60}{60}\selectfont''};}
#+LATEX_HEADER: % select a colour for the shading
#+LATEX_HEADER: %\definecolor{shadecolor}{named}{gray}
#+LATEX_HEADER: % wrap everything in its own environment
#+LATEX_HEADER: \newenvironment{shadequote}%
#+LATEX_HEADER: {\begin{quote}\openquote}
#+LATEX_HEADER: {\hfill\closequote\end{quote}}
#+LATEX_HEADER: 
#+LATEX_HEADER: \newcommand{\xelatex}{\XeLaTeX\xspace} 
#+LATEX_HEADER: \newcommand{\latex}{\LaTeX\xspace}
#+LATEX_HEADER: 
#+LATEX_HEADER: %\newglossary[<log-ext>]{<name>}{<in-ext>}{<out-ext>}{<title>}[<counter>]
#+LATEX_HEADER: %\newglossary[alg]{atom}{aot}{atn}{Zeichen-Ebene}
#+LATEX_HEADER: %\newglossary[slg]{sets}{sot}{stn}{Zeichensatz-Ebene}
#+LATEX_HEADER: %\newglossary[ulg]{unicode-specific}{uot}{utn}{Unicode-Spezifisches}
#+LATEX_HEADER: 
#+LATEX_HEADER: %\makeglossaries
#+LATEX_HEADER: %\loadglsentries{glossar}
#+LATEX_HEADER: % For BIBER
#+LATEX_HEADER: \DeclareSourcemap{
#+LATEX_HEADER:  \maps[datatype=bibtex, overwrite]{
#+LATEX_HEADER:    \map{
#+LATEX_HEADER:      \step[fieldset=language, null] % exclude bib language field from printing
#+LATEX_HEADER:      \step[fieldset=month, null] 
#+LATEX_HEADER:      \step[fieldset=pagetotal, null] 
#+LATEX_HEADER:    }
#+LATEX_HEADER:  }
#+LATEX_HEADER: }
#+LATEX_HEADER: \newcommand\mpDr[1]{\marginpar{\fontspec[Scale=0.7]{Droid Sans}#1}}
#+LATEX_HEADER: \newcommand\zb{z. B.}
#+LATEX_HEADER: \newcommand\di{d. I.}
#+LATEX_HEADER: 
#+LATEX_HEADER: %Elision in citation ... took so long to find this, don't know if this the best way :(
#+LATEX_HEADER: \newcommand*\elide{\textup{[\dots]}\xspace}
#+LATEX_HEADER: % Using "[" and "]" in the pre/postnote of citation seems a big problem, therefore new command for [sic]
#+LATEX_HEADER: \newcommand*\sic{\textup{[sic]}\xspace}
#+LATEX_HEADER: 
#+LATEX_HEADER: \hyphenation{dash}
#+LATEX_HEADER: \newfontfamily\dejavus[Mapping=tex-ansi]{DejaVu Sans}
#+LATEX_HEADER: \newfontfamily\scpro[Mapping=tex-ansi]{Source Code Pro}
#+LATEX_HEADER: \newfontfamily\linmono[Mapping=tex-ansi]{Linux Libertine Mono}
#+LATEX_HEADER: \newfontfamily\linansi[Mapping=tex-ansi]{Linux Libertine}
#+LATEX_HEADER: \newcommand{\mysinglespacing}{%
#+LATEX_HEADER:   \setstretch{1}% no correction afterwards
#+LATEX_HEADER: }
#+LATEX_HEADER: \lstnewenvironment{my-inlst}{\lstset{basicstyle=\small\ttfamily\setstretch{1},language=bash}}{}
#+LATEX_HEADER:  \newcommand*{\inlst}{\lstinline[basicstyle=\small\ttfamily\setstretch{1},language=bash,breaklines=true]}
#+LATEX_HEADER: %\newcommand{\inlst}[1]{%
#+LATEX_HEADER: %   \lstinline[basicstyle=\small\ttfamily\setstretch{1},language=bash]!#1!
#+LATEX_HEADER: %}
#+LATEX_HEADER: \newcommand{\stylst}{basicstyle=\small\ttfamily\setstretch{1}}
#+LATEX_HEADER: 
#+LATEX_HEADER: 
#+LATEX_HEADER: 
#+LATEX_HEADER: \usepackage{infobox} %thx to  https://github.com/lkiesow/thesis-latex/blob/master/tex/latex/infobox/infobox.sty              
#+LATEX_HEADER: %%%% Custom Command for floating Infoboxes
#+LATEX_HEADER: %%%% usage: \infobox{<title>}{<text>}
#+LATEX_HEADER: %\usepackage{picins} funktioniert nicht gut mit Liste (float-Umgebung) -- jetzt ohne Float mit infobox-package                
#+LATEX_HEADER: \newcommand{\infobox}[2]{
#+LATEX_HEADER:     \parpic(0.34\textwidth,0pt)[lf]{
#+LATEX_HEADER:         \parbox[b]{0.32\textwidth}{
#+LATEX_HEADER:              {\bf #1}  \small{{{#2}}}
#+LATEX_HEADER:         }
#+LATEX_HEADER:     }
#+LATEX_HEADER:     \bigskip
#+LATEX_HEADER: }

# Local Variables:
# zotero-collection: #("4" 0 1 (name "ChinGrammar"))
# End:
